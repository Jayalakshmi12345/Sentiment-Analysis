{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment analysis LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jayalakshmi12345/Sentiment-Analysis/blob/main/sentiment_analysis_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aim:**\n",
        "To implement sentiment analysis on movie reviews using LSTM .\n",
        "\n",
        "\n",
        "**Description:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Sentiment analysis is widely used, especially as a part of social media analysis for any\n",
        "domain, be it a business, a recent movie, or a product launch, to understand its reception by\n",
        "the people and what they think of it based on their opinions or sentiment!\n",
        "*   **RNN** is a type of supervised deep learning algorithm. Here, the neurons are connected to themselves through time. The idea behind RNN is to remember what information was there in the previous neurons so that these neurons could pass information to themselves in the future for further analysis. It means that the information from a specific time instance (t1) is used as an input for the next time instance(t2). This is the idea behind RNN.\n",
        "\n",
        "\n",
        "*   **LSTM**  is an updated version of Recurrent Neural Network to overcome the vanishing gradient problem. Below is the architecture of LSTM \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v4W7nrW7Kd1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procedure:**\n",
        "\n",
        "Step 1: Importing required libraries.\n",
        "\n",
        "Step 2: Loading the dataset.\n",
        "\n",
        "Step 3: Checking for null values in the dataset.\n",
        "\n",
        "Step 4: Cleaning the data. It includes removing the special characters, digits, unnecessary symbols, and stop words. Also, it is required to convert the words to their root form for easy interpretation.\n",
        "\n",
        "\n",
        "Step 5: Encoding the target variable using ‘Label Encoder’ from the ‘sklearn’ library.\n",
        "\n",
        "Step 6: Tokenizing and converting the reviews into numerical vectors.\n",
        "\n",
        "Step 7: Building the LSTM model using the ‘Keras’ library. This step involves model initialization, adding required LSTM layers, and model compilation\n",
        "\n",
        "Step 8: Splitting the data into training and testing data.\n",
        "\n",
        "Step 9: Training the model using training data.\n",
        "\n",
        "Step 10: Evaluating the model.\n"
      ],
      "metadata": {
        "id": "YqXjzTMPauId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code:**"
      ],
      "metadata": {
        "id": "gS-x12RpLuSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1**: Importing required libraries."
      ],
      "metadata": {
        "id": "N42MAJY7xyzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqCF8EIsv9ms"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from collections import Counter\n",
        "import wordcloud\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split \n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "lxLt_pwL1qmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "flQKBVvV44QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "vuSdfFY14_6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "MBcwlNND5Knj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "mGf15Wfq555l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Loading the dataset."
      ],
      "metadata": {
        "id": "p6DKW3ap65PZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url='/content/imdb-dataset-of-50k-movie-reviews.zip'"
      ],
      "metadata": {
        "id": "jwVChoE16A4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(url)"
      ],
      "metadata": {
        "id": "uLxfPgzp6XcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "sg3FCwgg6bBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Checking for null values in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "jhfaIp7t69aV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if there are any null values\n",
        "data_v1= df[['review','sentiment']]\n",
        "data_v1.isnull().sum()"
      ],
      "metadata": {
        "id": "3nXOHzBP6dn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Cleaning the data. It includes removing the special characters, digits, unnecessary symbols, and stop words. Also, it is required to convert the words to their root form for easy interpretation."
      ],
      "metadata": {
        "id": "Tedbfd2z7uYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "wytaSJ31-GCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaning(df, stop_words):\n",
        "    df['review'] = df['review'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
        "    # Replacing the special characters\n",
        "    df['review'] = df['review'].str.replace('[^ws]', '')\n",
        "    # Replacing the digits/numbers\n",
        "    df['review'] = df['review'].str.replace('d', '')\n",
        "    # Removing stop words\n",
        "    df['review'] = df['review'].apply(lambda x:' '.join(x for x in x.split() if x not in stop_words))\n",
        "    # Lemmatization\n",
        "    df['review'] = df['review'].apply(lambda x:' '.join([Word(x).lemmatize() for x in x.split()]))\n",
        "    return df\n",
        "stop_words = stopwords.words('english')\n",
        "data_v1 = cleaning(data_v1, stop_words)"
      ],
      "metadata": {
        "id": "vpbtRaWm7Nyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:** Encoding the target variable using ‘Label Encoder’ from the ‘sklearn’ library."
      ],
      "metadata": {
        "id": "U6NgKIWGA3Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoded the target column\n",
        "lb=LabelEncoder()\n",
        "data_v1['sentiment'] = lb.fit_transform(data_v1['sentiment'])\n",
        "data_v1['sentiment']"
      ],
      "metadata": {
        "id": "_0hw2t4iAIqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:** Tokenizing and converting the reviews into numerical vectors."
      ],
      "metadata": {
        "id": "omX0E5kQBBhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=500, split=' ') \n",
        "tokenizer.fit_on_texts(data_v1['review'].values)\n",
        "X = tokenizer.texts_to_sequences(df['review'].values)\n",
        "X = pad_sequences(X)"
      ],
      "metadata": {
        "id": "QnnLHZiwA9Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:** Building the LSTM model using the ‘Keras’ library. This step involves model initialization, adding required LSTM layers, and model compilation"
      ],
      "metadata": {
        "id": "D_3868PMBfaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(500, 120, input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(176, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bat5bZAnBNiU",
        "outputId": "212073ff-efaf-4740-a42e-123f021b7f73"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 12, 120)           60000     \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 12, 120)          0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 176)               209088    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 354       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 269,442\n",
            "Trainable params: 269,442\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8:** Splitting the data into training and testing data."
      ],
      "metadata": {
        "id": "XH9Vz36yCcB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data into training and testing\n",
        "y=pd.get_dummies(data_v1['sentiment'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)"
      ],
      "metadata": {
        "id": "etvxTwtkChfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9:** Training the model using training data."
      ],
      "metadata": {
        "id": "-WrdvenoCuF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=40\n",
        "model.fit(X_train, y_train, epochs = 10, batch_size=batch_size, verbose = 'auto')"
      ],
      "metadata": {
        "id": "HrL7_CZRC9Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 10:** Evaluating the model"
      ],
      "metadata": {
        "id": "xVMb2vkMDKZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "653Nke6XDQ2c",
        "outputId": "e9f5d939-b89e-4a92-b588-f3373233c893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 4s 9ms/step - loss: 0.6934 - accuracy: 0.4953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6934447884559631, 0.4952666759490967]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result:**\n",
        "Sentiment analysis using LSTM is implemented successfully."
      ],
      "metadata": {
        "id": "SXpA4rHzL5BK"
      }
    }
  ]
}